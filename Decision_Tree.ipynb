{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree"
      ],
      "metadata": {
        "id": "40-BSxHbP1a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "  - Answer: A Decision Tree is a type of supervised learning algorithm used for both classification and regression tasks. It's a tree-like model that splits data into subsets based on the values of input features.\n",
        "  \n",
        "  How Decision Trees Work in Classification\n",
        "    1. Root Node: The algorithm starts with a root node representing the entire dataset.\n",
        "    2. Splitting: The algorithm selects the best feature to split the data into subsets based on a splitting criterion (e.g., Gini impurity or entropy).\n",
        "    3. Child Nodes: Each subset of data is assigned to a child node, and the process is repeated recursively until a stopping criterion is met (e.g., all instances in a node belong to the same class).\n",
        "    4. Leaf Nodes: The final nodes in the tree are called leaf nodes, which represent the predicted class labels.\n",
        "    5. Prediction: To classify a new instance, the algorithm traverses the tree from the root node to a leaf node based on the feature values of the instance.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "  - Answer: Impurity measures are used in Decision Trees to determine the best split for a node. Two commonly used impurity measures are Gini Impurity and Entropy.\n",
        "\n",
        "    Gini Impurity\n",
        "    - Definition: Gini Impurity measures the probability of incorrectly classifying a randomly chosen instance from a node if it were randomly labeled according to the class distribution of the node.\n",
        "    - Formula: Gini Impurity is calculated as 1 - Σ (p_i^2), where p_i is the proportion of instances in the node that belong to class i.\n",
        "    - Range: Gini Impurity ranges from 0 (pure node) to 1 (impure node).\n",
        "\n",
        "    Entropy Impurity\n",
        "    - Definition: Entropy measures the uncertainty or randomness of a node. It represents the amount of information needed to specify the class of an instance in the node.\n",
        "    - Formula: Entropy is calculated as - Σ (p_i * log2(p_i)), where p_i is the proportion of instances in the node that belong to class i.\n",
        "    - Range: Entropy ranges from 0 (pure node) to log2(k) (impure node), where k is the number of classes.\n",
        "\n",
        "    Impact on Splits in a Decision Tree\n",
        "    - Gini Impurity: When using Gini Impurity, the Decision Tree algorithm chooses the split that results in the largest reduction in Gini Impurity. This means that the algorithm prefers splits that result in more homogeneous child nodes.\n",
        "    - Entropy: When using Entropy, the Decision Tree algorithm chooses the split that results in the largest reduction in Entropy. This means that the algorithm prefers splits that result in more certain child nodes.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "  - Answer: Pre-Pruning and Post-Pruning are two techniques used to prevent overfitting in Decision Trees.\n",
        "    \n",
        "    Pre-Pruning\n",
        "    - Definition: Pre-Pruning involves stopping the growth of the Decision Tree before it perfectly fits the training data. This is done by specifying a stopping criterion, such as a maximum depth or a minimum number of instances per node.\n",
        "    - Practical Advantage: One practical advantage of Pre-Pruning is that it can reduce computational cost. By stopping the growth of the tree early, we can avoid unnecessary computations and reduce the risk of overfitting.\n",
        "    \n",
        "    Post-Pruning\n",
        "    - Definition: Post-Pruning involves growing the Decision Tree to its full depth and then removing branches that do not contribute significantly to the tree's performance. This is done by evaluating the tree's performance on a validation set and removing branches that do not improve the performance.\n",
        "    - Practical Advantage: One practical advantage of Post-Pruning is that it can improve model accuracy. By growing the tree to its full depth, we can capture complex interactions in the data, and then remove branches that are not useful, resulting in a more accurate model.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "  - Answer: Information Gain is a measure used in Decision Trees to determine the best split for a node. It calculates the reduction in impurity or uncertainty after splitting a node.\n",
        "  \n",
        "  Why Information Gain is Important\n",
        "    - Choosing the Best Split: Information Gain helps choose the best split by selecting the feature that results in the largest reduction in impurity or uncertainty.\n",
        "    - Feature Selection: Information Gain helps select the most informative features for splitting, which can improve the accuracy and efficiency of the Decision Tree.\n",
        "    - Tree Construction: By maximizing Information Gain, Decision Trees can construct a more optimal tree structure that captures the underlying relationships in the data.\n",
        "    - Reducing Uncertainty: By maximizing Information Gain, Decision Trees can reduce uncertainty and improve the accuracy of predictions.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "  - Answer: Decision Trees are widely used in various industries and domains due to their simplicity, interpretability, and effectiveness. Some common real-world applications include:\n",
        "    1. Credit Risk Assessment: Decision Trees are used to evaluate the creditworthiness of loan applicants based on their credit history, income, and other factors.\n",
        "    2. Medical Diagnosis: Decision Trees are used to diagnose diseases based on symptoms, medical history, and test results.\n",
        "    3. Customer Segmentation: Decision Trees are used to segment customers based on their behavior, demographics, and preferences.\n",
        "    4. Predictive Maintenance: Decision Trees are used to predict equipment failures and schedule maintenance based on sensor data and historical records.\n",
        "    5. Marketing and Sales: Decision Trees are used to identify potential customers, predict sales, and optimize marketing campaigns.\n",
        "  \n",
        "  Main Advantages of Decision Trees\n",
        "    1. Interpretability: Decision Trees are easy to understand and interpret, making them a popular choice for many applications.\n",
        "    2. Handling Categorical Features: Decision Trees can handle categorical features directly without requiring encoding.\n",
        "    3. Handling Missing Values: Decision Trees can handle missing values by using surrogate splits or treating missing values as a separate category.\n",
        "    4. Fast Training: Decision Trees are relatively fast to train compared to other machine learning algorithms.\n",
        "  \n",
        "  Main Limitations of Decision Trees\n",
        "    1. Overfitting: Decision Trees can overfit the training data, especially when the trees are deep or complex.\n",
        "    2. Instability: Small changes in the data can lead to large changes in the tree structure.\n",
        "    3. Limited Handling of Complex Relationships: Decision Trees can struggle to capture complex relationships between features.\n",
        "    4. Not Suitable for High-Dimensional Data: Decision Trees can become overly complex and prone to overfitting when dealing with high-dimensional data.\n"
      ],
      "metadata": {
        "id": "TgfnSfsjP5Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "# Print the model’s accuracy and feature importances\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for i, feature in enumerate(iris.feature_names):\n",
        "    print(f\"{feature}: {feature_importances[i]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KAfEBhYSFtu",
        "outputId": "58139b1f-f84f-4f33-9e1a-179f080b9251"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.000\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.000\n",
            "sepal width (cm): 0.017\n",
            "petal length (cm): 0.906\n",
            "petal width (cm): 0.077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#Load the Iris Dataset\n",
        "#Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "#(Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Decision Tree with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Fully grown Decision Tree\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.4f}\")\n",
        "print(f\"Accuracy with fully grown tree: {accuracy_full:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ah1zUT5rSfIg",
        "outputId": "6a529c24-21b0-4069-de9e-65efac7247e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0000\n",
            "Accuracy with fully grown tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# Load the California Housing dataset from sklearn\n",
        "# Train a Decision Tree Regressor\n",
        "# Print the Mean Squared Error (MSE) and feature importances\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X, y = california.data, california.target\n",
        "feature_names = california.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\\n\")\n",
        "\n",
        "# Print feature importances\n",
        "importances = regressor.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importance_df.to_string(index=False))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diF9XP8GS0W-",
        "outputId": "87e4aa07-6437-4afa-8d55-68d212447011"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n",
            "\n",
            "Feature Importances:\n",
            "   Feature  Importance\n",
            "    MedInc    0.528509\n",
            "  AveOccup    0.130838\n",
            "  Latitude    0.093717\n",
            " Longitude    0.082902\n",
            "  AveRooms    0.052975\n",
            "  HouseAge    0.051884\n",
            "Population    0.030516\n",
            " AveBedrms    0.028660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# Print the best parameters and the resulting model accuracy\n",
        "# (Include your Python code and output in the code box below.)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkwGBMW3TPx6",
        "outputId": "8e0e311b-11c0-415b-b332-6e2c1c9ac31e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy on Test Set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "*  Handle the missing values\n",
        "*  Encode the categorical features\n",
        "*  Train a Decision Tree model\n",
        "*  Tune its hyperparameters\n",
        "*  Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "- Answer:\n",
        "\n",
        "  Handling Missing Values\n",
        "    1. Identify Missing Values: Use pandas' isnull() function to identify missing values in the dataset.\n",
        "    2. Determine the Type of Missing Values: Determine whether the missing values are Missing At Random (MAR), Missing Completely At Random (MCAR), or Missing Not At Random (MNAR).\n",
        "    3. Choose an Imputation Method: Based on the type of missing values and the data distribution, choose an imputation method such as mean, median, mode, or a more advanced method like K-Nearest Neighbors (KNN) or Multiple Imputation by Chained Equations (MICE).\n",
        "    4. Impute Missing Values: Use the chosen imputation method to fill in the missing values.\n",
        "\n",
        "  Encoding Categorical Features\n",
        "    1. Identify Categorical Features: Identify the categorical features in the dataset.\n",
        "    2. Choose an Encoding Method: Choose an encoding method such as One-Hot Encoding (OHE), Label Encoding, or Ordinal Encoding based on the type of categorical feature and the model being used.\n",
        "    3. Encode Categorical Features: Use the chosen encoding method to transform the categorical features into numerical features.\n",
        "\n",
        "  Training a Decision Tree Model\n",
        "    1. Split the Data: Split the dataset into training and testing sets using train_test_split() from scikit-learn.\n",
        "    2. Train a Decision Tree Model: Train a Decision Tree model using DecisionTreeClassifier from scikit-learn.\n",
        "    3. Specify Hyperparameters: Specify the hyperparameters for the Decision Tree model, such as max_depth, min_samples_split, and min_samples_leaf.\n",
        "\n",
        "  Tuning Hyperparameters\n",
        "    1. Choose a Hyperparameter Tuning Method: Choose a hyperparameter tuning method such as Grid Search, Random Search, or Bayesian Optimization.\n",
        "    2. Define the Hyperparameter Grid: Define the hyperparameter grid to search over.\n",
        "    3. Perform Hyperparameter Tuning: Use the chosen hyperparameter tuning method to find the optimal hyperparameters for the Decision Tree model.\n",
        "\n",
        "  Evaluating Performance\n",
        "    1. Choose Evaluation Metrics: Choose evaluation metrics such as accuracy, precision, recall, F1-score, and ROC-AUC score.\n",
        "    2. Evaluate the Model: Evaluate the performance of the Decision Tree model using the chosen evaluation metrics.\n",
        "    3. Compare to Baseline Models: Compare the performance of the Decision Tree model to baseline models or other machine learning models.\n",
        "\n",
        "  Business Value\n",
        "    \n",
        "  The Decision Tree model can provide significant business value in the real-world setting by:\n",
        "\n",
        "    1. Improving Disease Diagnosis: The model can help healthcare professionals diagnose diseases more accurately and efficiently.\n",
        "    2. Reducing Costs: The model can help reduce costs associated with misdiagnosis, unnecessary tests, and treatments.\n",
        "    3. Improving Patient Outcomes: The model can help improve patient outcomes by enabling early detection and treatment of diseases.\n",
        "    4. Enhancing Clinical Decision Support: The model can provide clinical decision support to healthcare professionals, enabling them to make more informed decisions.\n"
      ],
      "metadata": {
        "id": "bg8bGfPvUDej"
      }
    }
  ]
}